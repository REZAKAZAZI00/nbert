{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/REZAKAZAZI00/nbert/blob/main/nbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TfJHCWbJ975",
        "outputId": "e3210958-28eb-4cd5-8377-94fe97125b21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.6\n"
          ]
        }
      ],
      "source": [
        "pip install transformers datasets torch accelerate evaluate scikit-learn nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XJJwZ-ZPMmH8",
        "outputId": "3fdf9a6d-bf30-4c51-f549-d482c56028bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Optional\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    AutoConfig,\n",
        "    set_seed,\n",
        ")\n",
        "from datasets import load_dataset, DatasetDict, load_from_disk\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "import evaluate\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gdV_9zERM4X1"
      },
      "outputs": [],
      "source": [
        "# Config\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "DATASET_NAME = \"nbertagnolli/counsel-chat\"  # Hugging Face dataset id used in your notebook\n",
        "NRC_PATH = \"NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"  # must exist locally\n",
        "OUTPUT_DIR = \"./nbert_counsel_run\"\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 16\n",
        "LR = 3e-5\n",
        "EPOCHS = 10\n",
        "SEED = 42\n",
        "set_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_yTRjH9QM7W-"
      },
      "outputs": [],
      "source": [
        "# Emotions considered by NRC (common set)\n",
        "NRC_EMOTIONS = [\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"sadness\",\"surprise\",\"trust\",\"positive\",\"negative\"]\n",
        "\n",
        "# -----------------------------\n",
        "# Helper: load NRC lexicon\n",
        "# -----------------------------\n",
        "def load_nrc_lexicon(path: str) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Load NRC word-level lexicon file into dict: word -> list of emotions it maps to.\n",
        "    Expected format (NRC file): word \\t emotion \\t association(0/1)\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"NRC lexicon not found at {path}. Please download and place it there.\")\n",
        "    lex = {}\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) != 3:\n",
        "                continue\n",
        "            word, emotion, assoc = parts\n",
        "            if assoc == '1':\n",
        "                lex.setdefault(word, []).append(emotion)\n",
        "    return lex\n",
        "\n",
        "nrc_lex = load_nrc_lexicon(NRC_PATH)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def sentence_to_nrc_vector(text: str, lex: Dict[str,List[str]]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Return an emotion count vector of length len(NRC_EMOTIONS)\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    vec = np.zeros(len(NRC_EMOTIONS), dtype=float)\n",
        "    for t in tokens:\n",
        "        t = re.sub(r'\\W+', '', t)\n",
        "        if not t:\n",
        "            continue\n",
        "        t_lem = lemmatizer.lemmatize(t)\n",
        "        emos = lex.get(t_lem) or lex.get(t)\n",
        "        if emos:\n",
        "            for e in emos:\n",
        "                if e in NRC_EMOTIONS:\n",
        "                    vec[NRC_EMOTIONS.index(e)] += 1.0\n",
        "    # normalize (if any)\n",
        "    s = vec.sum()\n",
        "    if s > 0:\n",
        "        vec = vec / s\n",
        "    return vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "J_tde9EtNXCc"
      },
      "outputs": [],
      "source": [
        "class CounselChatEmotionDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, tokenizer, max_len=256, nrc_lexicon=None):\n",
        "        self.ds = hf_dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.nrc = nrc_lexicon\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.ds[idx]\n",
        "        # Adjust this depending on dataset fields. We'll try common candidates.\n",
        "        text = None\n",
        "        for key in [\"text\", \"utterance\", \"content\", \"transcript\", \"dialogue\", \"message\"]:\n",
        "            if key in row:\n",
        "                text = row[key]\n",
        "                break\n",
        "        if text is None:\n",
        "            # fallback: stringify whole row\n",
        "            text = \" \".join(str(v) for v in row.values())\n",
        "\n",
        "        # create NRC vector\n",
        "        nrc_vec = sentence_to_nrc_vector(text, self.nrc) if self.nrc else np.zeros(len(NRC_EMOTIONS), dtype=float)\n",
        "\n",
        "        tok = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=None,\n",
        "        )\n",
        "        # Label: try common fields\n",
        "        label = None\n",
        "        for lab_key in [\"label\", \"emotion\", \"emotion_label\", \"emo\"]:\n",
        "            if lab_key in row:\n",
        "                label = row[lab_key]\n",
        "                break\n",
        "        # If dataset is unlabeled, we will set label to -1\n",
        "        if label is None:\n",
        "            label = -1\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": tok[\"input_ids\"],\n",
        "            \"attention_mask\": tok[\"attention_mask\"],\n",
        "            \"nrc_vec\": nrc_vec.astype(np.float32),\n",
        "            \"label\": int(label) if isinstance(label, (int, np.integer)) else label\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_IaRXdmuNuIC"
      },
      "outputs": [],
      "source": [
        "class NbertLikeModel(nn.Module):\n",
        "    def __init__(self, model_name: str, nrc_dim: int, num_labels: int):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        hidden_size = self.bert.config.hidden_size  # typically 768\n",
        "        self.nrc_dim = nrc_dim\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        # fuse CLS embedding with NRC vector (project NRC to hidden_size then combine)\n",
        "        self.nrc_proj = nn.Linear(nrc_dim, hidden_size)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_size, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, nrc_vec=None, labels=None):\n",
        "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        cls = bert_out.last_hidden_state[:,0,:]  # [B, hidden]\n",
        "        if nrc_vec is None:\n",
        "            nrc_vec = torch.zeros((cls.size(0), self.nrc_dim), device=cls.device)\n",
        "        else:\n",
        "            nrc_vec = nrc_vec.to(cls.device).float()\n",
        "\n",
        "        nrc_proj = self.nrc_proj(nrc_vec)  # [B, hidden]\n",
        "        cat = torch.cat([cls, nrc_proj], dim=-1)  # [B, hidden*2]\n",
        "\n",
        "        logits = self.classifier(cat)\n",
        "        loss = None\n",
        "        if labels is not None and labels.dim() == 1:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": logits}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UTwLhltCTG5i",
        "outputId": "ac0e95e9-232b-4bf6-bbf7-cf1036cddd3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Collecting transformers>=4.30.0\n",
            "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.30.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.30.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.30.0) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.30.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.30.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.30.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.30.0) (2025.11.12)\n",
            "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.2\n",
            "    Uninstalling transformers-4.57.2:\n",
            "      Successfully uninstalled transformers-4.57.2\n",
            "Successfully installed transformers-4.57.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "c681027945fe477a993087344b35fc5c",
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pip install -U \"transformers>=4.30.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KPVrXc8INyBu"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "    return {\"accuracy\": acc, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA21hIAeN7-_"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(\"Loading dataset...\", DATASET_NAME)\n",
        "raw = load_dataset(DATASET_NAME)\n",
        "\n",
        "# این دیتاست فقط یک split به نام train دارد، پس خودمان train/test split می‌سازیم\n",
        "if \"train\" in raw and len(raw) == 1:\n",
        "    print(\"Dataset has only 'train' split. Creating train/test split...\")\n",
        "    ds = raw[\"train\"].train_test_split(test_size=0.2, seed=SEED)\n",
        "else:\n",
        "    ds = raw\n",
        "\n",
        "# ساخت train و test\n",
        "train_ds = CounselChatEmotionDataset(ds[\"train\"], tokenizer, max_len=MAX_LEN, nrc_lexicon=nrc_lex)\n",
        "val_ds   = CounselChatEmotionDataset(ds[\"test\"], tokenizer, max_len=MAX_LEN, nrc_lexicon=nrc_lex)\n",
        "\n",
        "# اگر دیتاست validation داشت، آن را جایگزین test می‌کنیم\n",
        "if \"validation\" in ds:\n",
        "    val_ds = CounselChatEmotionDataset(ds[\"validation\"], tokenizer, max_len=MAX_LEN, nrc_lexicon=nrc_lex)\n",
        "\n",
        "# ---------------------------\n",
        "# تعیین تعداد لیبل‌ها\n",
        "# ---------------------------\n",
        "num_labels = 2  # پیش‌فرض\n",
        "try:\n",
        "    hf_features = ds[\"train\"].features\n",
        "    if \"label\" in hf_features:\n",
        "        lab = hf_features[\"label\"]\n",
        "        if hasattr(lab, \"num_classes\"):\n",
        "            num_labels = int(lab.num_classes)\n",
        "        else:\n",
        "            labels = ds[\"train\"][\"label\"]\n",
        "            num_labels = len(set(labels))\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "print(\"Num labels inferred:\", num_labels)\n",
        "\n",
        "# ---------------------------\n",
        "# مدل\n",
        "# ---------------------------\n",
        "model = NbertLikeModel(MODEL_NAME, nrc_dim=len(NRC_EMOTIONS), num_labels=num_labels)\n",
        "\n",
        "# ---------------------------\n",
        "# Collate Function\n",
        "# ---------------------------\n",
        "def collate_fn(batch):\n",
        "    input_ids = [b[\"input_ids\"] for b in batch]\n",
        "    attention_masks = [b[\"attention_mask\"] for b in batch]\n",
        "    nrc_vecs = np.stack([b[\"nrc_vec\"] for b in batch])\n",
        "    labels = [b[\"label\"] for b in batch]\n",
        "\n",
        "    tok = tokenizer.pad({\"input_ids\": input_ids, \"attention_mask\": attention_masks},\n",
        "                        return_tensors=\"pt\")\n",
        "    tok[\"nrc_vec\"] = torch.tensor(nrc_vecs, dtype=torch.float32)\n",
        "    tok[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
        "    return tok\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset Wrapper\n",
        "# ---------------------------\n",
        "class HFDatasetWrapper(torch.utils.data.Dataset):\n",
        "    def __init__(self, ds_obj):\n",
        "        self.ds_obj = ds_obj\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds_obj)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.ds_obj[idx]\n",
        "\n",
        "train_wrapper = HFDatasetWrapper(train_ds)\n",
        "val_wrapper   = HFDatasetWrapper(val_ds)\n",
        "\n",
        "# ---------------------------\n",
        "# Training Arguments\n",
        "# ---------------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=LR,\n",
        "    weight_decay=0.04,\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    push_to_hub=False,\n",
        "    fp16=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Custom Trainer\n",
        "# ---------------------------\n",
        "class SimpleTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        nrc_vec = inputs.pop(\"nrc_vec\")\n",
        "        outputs = model(**inputs, nrc_vec=nrc_vec, labels=labels)\n",
        "        loss = outputs[\"loss\"]\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "trainer = SimpleTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_wrapper,\n",
        "    eval_dataset=val_wrapper,\n",
        "    data_collator=collate_fn,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=None\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Train\n",
        "# ---------------------------\n",
        "trainer.train()\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluation\n",
        "# ---------------------------\n",
        "print(\"Evaluating...\")\n",
        "preds_output = trainer.predict(val_wrapper)\n",
        "logits = preds_output.predictions\n",
        "y_pred = np.argmax(logits, axis=1)\n",
        "y_true = preds_output.label_ids\n",
        "\n",
        "print(\"Classification report:\")\n",
        "try:\n",
        "    print(classification_report(y_true, y_pred, digits=4))\n",
        "except Exception as e:\n",
        "    print(\"Could not print classification report:\", e)\n",
        "\n",
        "# ---------------------------\n",
        "# Save\n",
        "# ---------------------------\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "print(\"Done. Model saved to\", OUTPUT_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOPOwOOJ0cqbqiCrk8FCAM3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}